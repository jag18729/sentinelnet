# Journal: 2026-02-11 — Infrastructure Day

## Summary

Full-day session standing up the SentinelNet ML infrastructure across the mesh. XPS (RTX 4060 Ti) as training GPU, pi2 (Pi 5, 16GB) as ONNX inference server, ThinkStation as control plane. Hit significant WSL2 stability issues on XPS that required deep troubleshooting.

## What Got Done

### ML Infrastructure
- **XPS Training Setup:** CUDA 12.4, PyTorch 2.6.0+cu124, SentinelNet repo cloned, all deps installed (torch, scikit-learn, wandb, optuna, onnx, cleverhans, ART, fastapi)
- **Model Verified:** SentinelNet 1D-CNN + BiLSTM, 768K params, input (32,78) → output (32,15) on CUDA
- **Pi2 Inference Server:** ONNX Runtime 1.24.1 (ARM64), FastAPI on port 8000, dummy model tested end-to-end
- **CICIDS2017 Dataset:** Downloaded from `c01dsnap/CIC-IDS2017` (HuggingFace), 4 parquet shards, 444MB, 2.83M rows, 78 features + Label

### Network & Connectivity
- **Tailscale Mesh:** All hosts connected (pi0, pi1, pi2, XPS, ThinkStation, Mac)
- **SSH Mesh:** Passwordless SSH across all devices
- **Direct LAN SSH to XPS:** Windows port proxy (22 → WSL), firewall rule added, 1.4ms latency
- **DNS:** `/etc/hosts` entries on all hosts for mesh hostnames
- **Tmux:** Standardized config across all hosts (Ctrl-a prefix, host shortcuts)
- **MOTD:** Tmux shortcuts displayed on login for all Pis

### Documentation
- Obsidian docs: Host Inventory, SentinelNet ML Project, pi2 Services
- MEMORY.md comprehensive update

## Lessons Learned

### 1. WSL2 + NordVPN = Unstable

**Problem:** XPS WSL2 hung repeatedly (4+ times), requiring full Windows reboots. `HCS_E_CONNECTION_TIMEOUT` error on `wsl -d Ubuntu-24.04`.

**Root Causes (multiple, compounding):**
- **NordLynx adapter** (WireGuard-based) conflicts with Hyper-V virtual networking. Even when "disconnected," the adapter stays active at 10.5.0.2 and interferes with the WSL2 VM's network bridge.
- **`/etc/resolv.conf` symlink loop:** WSL boots trying to symlink `/etc/resolv.conf`, but Tailscale had set the immutable flag (`chattr +i`). WSL retried thousands of times per boot, filling dmesg with errors and destabilizing the VM under load.
- **Stale services:** MySQL (400MB), Ollama, Samba, iperf3, snap, atop all running unnecessarily, adding memory pressure and I/O contention to an already fragile VM.
- **DXG GPU passthrough errors:** `dxgkio_query_adapter_info: Ioctl failed` on every boot — NVIDIA driver mismatch between Windows host and WSL2 kernel (5.15.167, ancient).
- **Old WSL kernel:** 5.15.167 is years behind current (6.x). Known stability issues with Hyper-V memory ballooning and network adapters.

**Fixes Applied:**
1. `generateResolvConf = false` in `/etc/wsl.conf` — stops the symlink loop
2. `chattr -i /etc/resolv.conf` — removes immutable flag
3. Disabled 10+ unnecessary services (mysql, ollama, smbd, nmbd, etc.)
4. Manual DNS: `nameserver 8.8.8.8` in `/etc/resolv.conf`
5. Direct LAN SSH as backup path (bypasses Tailscale relay entirely)

**Still Needed:**
- Disable NordVPN network adapters in Windows (`Disable-NetAdapter`)
- `wsl --update` to get 6.x kernel (failed with error 1603, retry later)
- Consider fresh Windows + WSL install (XPS has "too much garbage")

**Takeaway:** If you run Tailscale inside WSL2, do NOT let WSL manage resolv.conf. Set `generateResolvConf = false` from day one. And never run a competing WireGuard VPN (NordVPN/NordLynx) on the Windows host — they fight over routing tables and destabilize Hyper-V networking.

### 2. HuggingFace `datasets` Library is Fragile

**Problem:** `load_dataset()` crashed mid-download (WSL hung), then couldn't recover. Corrupt cache entries caused `NonMatchingSplitsSizesError` and `FileNotFoundError` on every retry, even after clearing `~/.cache/huggingface/datasets/`.

**Why:** The library writes to both `datasets/` (arrow cache) and `hub/` (blob cache). Clearing one without the other leaves inconsistent state. The `force_redownload` flag doesn't always force re-fetching blobs. And the httpx client sometimes enters a "closed" state after network interruptions.

**Fix:** Skip the `datasets` library entirely for large downloads. Use the HuggingFace parquet API:
```
https://huggingface.co/api/datasets/<owner>/<name>/parquet/default/train/<N>.parquet
```
Then `wget` the files directly. Faster, resumable, no cache corruption risk.

**Takeaway:** For production ML pipelines, download raw parquet files and load with pandas/pyarrow. The `datasets` library is convenient for exploration but too fragile for infrastructure.

### 3. Always Have a Backup SSH Path

**Problem:** Tailscale relay between ThinkStation ↔ XPS had 1-3s latency (DERP relay "lax"). When WSL hung, the relay connection died completely — no way to diagnose remotely.

**Fix:** Set up Windows port proxy to forward port 22 to WSL SSH:
```powershell
netsh interface portproxy add v4tov4 listenport=22 listenaddress=0.0.0.0 connectport=22 connectaddress=localhost
New-NetFirewallRule -DisplayName "SSH WSL" -Direction Inbound -LocalPort 22 -Protocol TCP -Action Allow
```

Now we have two paths:
- **Direct LAN:** `ssh rjgar@192.168.2.71` (1.4ms) — works when on home network
- **Tailscale relay:** `ssh rjgar@100.73.127.58` — works anywhere

**Takeaway:** Never rely on a single access path to a remote machine, especially one running inside a VM (WSL2). The port proxy survives WSL restarts and gives sub-2ms access on LAN.

### 4. Clean Dataset Source Matters

**Problem:** Original dataset source (`rdpahalavan/CIC-IDS2017`) had 55 raw CSV files requiring extensive preprocessing. Download was 17GB+ and took forever.

**Fix:** Found `c01dsnap/CIC-IDS2017` — same data, pre-cleaned into 4 parquet shards (444MB total, 2.83M rows). Downloaded in 14 seconds on WiFi.

**Takeaway:** Always check for cleaned/parquet versions of research datasets on HuggingFace before downloading raw CSVs. The time savings compound — less preprocessing code, less storage, faster iteration.

## Pipeline Status

```
XPS (Training)          pi2 (Inference)         pi1 (Monitoring)
├── CUDA 12.4 ✅        ├── ONNX Runtime ✅      ├── Grafana ✅
├── PyTorch 2.6 ✅      ├── FastAPI :8000 ✅     ├── Prometheus ✅
├── SentinelNet ✅      ├── Dummy model ✅       └── Loki ✅
├── CICIDS2017 ✅       └── Awaiting real model
└── Ready to train
```

## Next Steps
1. Train baseline SentinelNet model on XPS
2. Export to ONNX, deploy to pi2
3. Set up wandb experiment tracking
4. Implement adversarial attacks (FGSM, PGD, C&W)
5. Connect pi0 syslog → pi2 inference pipeline
