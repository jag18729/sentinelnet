# Journal: 2026-02-11 Night — Full Fleet Observability

## Summary

The SentinelNet pipeline doesn't end at the model. If you're deploying ML inference on edge hardware and routing predictions through a distributed network, you need to see everything: host health, service availability, log aggregation, and alerting. Tonight was about closing that gap across all 5 mesh hosts.

## Why This Matters

SentinelNet's production pipeline spans multiple hosts with different roles:
- **XPS** trains the model (GPU)
- **pi2** serves inference (ONNX on ARM64)
- **pi1** runs Grafana/Prometheus (visualization)
- **pi0** collects syslog from the firewall (data source)
- **ThinkStation** is the control plane

If any of these go down silently, the pipeline breaks. Consistent monitoring across the fleet means we catch failures before they cascade, and we have the telemetry to debug when things go sideways (which they will, especially on WSL2).

## What Got Done

Onboarded pi2, ThinkStation, and XPS to match pi0/pi1's monitoring stack:

**Per host:**
- **Datadog Agent** — infrastructure metrics, process monitoring, log collection
- **Prometheus node_exporter** — CPU, memory, disk, network metrics scraped centrally
- **Vector** — structured log shipping to Loki (queryable via Grafana)

**Prometheus targets added:**
- Node exporters for all new hosts
- ICMP probes for reachability
- HTTP probe for SentinelNet inference API

All 5 hosts reporting green by end of session.

## wandb Dashboard

Synced the offline training run to the cloud dashboard. Full convergence curve through Epoch 26: 98.74% train accuracy, 98.76% validation accuracy, loss 0.031. No overfitting. Best model checkpoint saved.

## Technical Notes

**WSL2 port exposure:** Services inside WSL don't bind to the Windows host's LAN interface. Each port needs a Windows `netsh portproxy` rule forwarding to the WSL VM. The VM IP changes on reboot, so these aren't fully persistent without a startup script.

**Persistent DNS on WSL2:** `resolvectl` settings are ephemeral. A systemd drop-in config survives reboots. We point to our own DNS instances first, public resolvers as fallback.

**XPS cleanup:** Removed McAfee and NordVPN, both of which were interfering with WSL2 networking. Significantly more stable after removal.

## Takeaways

1. Observability isn't optional for distributed ML pipelines. You can't debug what you can't see.
2. WSL2 as a training host works but requires deliberate network configuration; it's not plug-and-play for LAN services.
3. The monitoring stack (Datadog + Prometheus + Vector + Loki) gives us three independent views: infrastructure metrics, time-series scraping, and structured logs. Redundancy in observability is the point.

## Next Steps
1. Restart training on XPS (interrupted by reboot, checkpoint saved)
2. Export trained model to ONNX, deploy to pi2
3. Build Grafana dashboard panels for the new hosts
4. Connect firewall syslog → inference pipeline
