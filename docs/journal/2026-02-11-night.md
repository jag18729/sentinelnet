# Journal: 2026-02-11 Night — Full Fleet Observability

## Summary

The SentinelNet pipeline doesn't end at the model. If you're deploying ML inference on edge hardware and routing predictions through a distributed network, you need to see everything: host health, service availability, log aggregation, and alerting. Tonight was about closing that gap across all 5 mesh hosts.

## Why This Matters

SentinelNet's production pipeline spans multiple hosts with different roles:
- **XPS** trains the model (GPU)
- **pi2** serves inference (ONNX on ARM64)
- **pi1** runs Grafana/Prometheus (visualization)
- **pi0** collects syslog from the PA-220 (data source)
- **ThinkStation** is the control plane

If any of these go down silently, the pipeline breaks. Consistent monitoring across the fleet means we catch failures before they cascade, and we have the telemetry to debug when things go sideways (which they will, especially on WSL2).

## What Got Done

Onboarded pi2, ThinkStation, and XPS to match pi0/pi1's monitoring stack:

**Per host:**
- **Datadog Agent** — infrastructure metrics, process monitoring, log collection
- **Prometheus node_exporter** — CPU, memory, disk, network metrics scraped by pi1
- **Vector** — structured log shipping to Loki (queryable via Grafana)

**Prometheus targets added:**
- Node exporters for pi2, ThinkStation, XPS
- ICMP probes for all three
- HTTP probe for SentinelNet API on pi2

**Datadog TCP checks on pi0:**
- SSH, node_exporter, and SentinelNet ports across all new hosts

All 5 hosts reporting green by end of session.

## wandb Dashboard

Synced the offline training run to wandb.ai. Dashboard shows full convergence curve through Epoch 26: 98.74% train accuracy, 98.76% validation accuracy, loss 0.031. No overfitting. Model checkpoint saved at `checkpoints/best.pt`.

https://wandb.ai/jag927-nasa/sentinelnet/runs/rp9sfghv

## Technical Notes

**WSL2 port exposure:** Services inside WSL don't bind to the Windows host's LAN interface. Each port needs a Windows `netsh portproxy` rule forwarding to the WSL VM IP. The VM IP changes on reboot, so these aren't fully persistent without a startup script.

**Persistent DNS on WSL2:** `resolvectl` settings are ephemeral. Drop-in config at `/etc/systemd/resolved.conf.d/dns.conf` survives reboots. We point to our own AdGuard instances first, Cloudflare as fallback.

**XPS cleanup:** Removed McAfee and NordVPN, both of which were interfering with WSL2 networking (DNS resolution failures, adapter conflicts). Significantly more stable after removal.

## Takeaways

1. Observability isn't optional for distributed ML pipelines. You can't debug what you can't see.
2. WSL2 as a training host works but requires deliberate network configuration; it's not plug-and-play for LAN services.
3. The monitoring stack (Datadog + Prometheus + Vector + Loki) gives us three independent views: infrastructure metrics, time-series scraping, and structured logs. Redundancy in observability is the point.

## Next Steps
1. Restart training on XPS (interrupted by reboot, checkpoint saved)
2. Export trained model to ONNX, deploy to pi2
3. Build Grafana dashboard panels for the new hosts
4. Connect PA-220 syslog → pi2 inference pipeline
